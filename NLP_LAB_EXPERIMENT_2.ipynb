{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP LAB EXPERIMENT 2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzM3XybcuCMN"
      },
      "source": [
        "NLP LAB EXPERIMENT 2\n",
        "1. TOKENIZATION USING **SPLIT** METHOD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3b9jWQJuMi5",
        "outputId": "f556f0c4-26b5-4b29-8de4-a4d8c242c2fc"
      },
      "source": [
        "# WORD TOKENIZATION\n",
        "text = '''My son, Mark, volunteered to help Cherie, a young runner at a local Special Olympics. Cherie was happy and enthusiastic. Mark encouraged her, kept her calm, and helped her know when it was time to line up for her race. When the starting pistol sounded, she took off like a lightning bolt, leaving her fellow racers behind. As she neared the finish line, she stopped, turned around and motioned for the other runners to hurry. She waited for them so they could all cross the finish line together.'''\n",
        "\n",
        "tokens = text.split()\n",
        "\n",
        "print(tokens)\n",
        "\n",
        "print(\"No.of Tokens:\",len(tokens))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['My', 'son,', 'Mark,', 'volunteered', 'to', 'help', 'Cherie,', 'a', 'young', 'runner', 'at', 'a', 'local', 'Special', 'Olympics.', 'Cherie', 'was', 'happy', 'and', 'enthusiastic.', 'Mark', 'encouraged', 'her,', 'kept', 'her', 'calm,', 'and', 'helped', 'her', 'know', 'when', 'it', 'was', 'time', 'to', 'line', 'up', 'for', 'her', 'race.', 'When', 'the', 'starting', 'pistol', 'sounded,', 'she', 'took', 'off', 'like', 'a', 'lightning', 'bolt,', 'leaving', 'her', 'fellow', 'racers', 'behind.', 'As', 'she', 'neared', 'the', 'finish', 'line,', 'she', 'stopped,', 'turned', 'around', 'and', 'motioned', 'for', 'the', 'other', 'runners', 'to', 'hurry.', 'She', 'waited', 'for', 'them', 'so', 'they', 'could', 'all', 'cross', 'the', 'finish', 'line', 'together.']\n",
            "No.of Tokens: 88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WCrpcZvwhGc",
        "outputId": "031ef247-bfa8-497d-a649-3f5f39357c9b"
      },
      "source": [
        "# SENTENCE TOKENIZATION\n",
        "text = '''My son, Mark, volunteered to help Cherie, a young runner at a local Special Olympics. Cherie was happy and enthusiastic. Mark encouraged her, kept her calm, and helped her know when it was time to line up for her race. When the starting pistol sounded, she took off like a lightning bolt, leaving her fellow racers behind. As she neared the finish line, she stopped, turned around and motioned for the other runners to hurry. She waited for them so they could all cross the finish line together.'''\n",
        "\n",
        "sentences = text.split('.')\n",
        "\n",
        "print(sentences)\n",
        "\n",
        "print(\"No.of sentences:\",len(sentences))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['My son, Mark, volunteered to help Cherie, a young runner at a local Special Olympics', ' Cherie was happy and enthusiastic', ' Mark encouraged her, kept her calm, and helped her know when it was time to line up for her race', ' When the starting pistol sounded, she took off like a lightning bolt, leaving her fellow racers behind', ' As she neared the finish line, she stopped, turned around and motioned for the other runners to hurry', ' She waited for them so they could all cross the finish line together', '']\n",
            "No.of sentences: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxnZjjpBwJUh"
      },
      "source": [
        "TOKENIZATION USING REGULAR EXPRESSIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNjdd6TmxINL"
      },
      "source": [
        "import re"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRfXA_msw7nr",
        "outputId": "9a11e12f-4609-46d1-a6f0-92464775050f"
      },
      "source": [
        "# WORD TOKENIZATION\n",
        "\n",
        "text = '''My son, Mark, volunteered to help Cherie, a young runner at a local Special Olympics. Cherie was happy and enthusiastic. Mark encouraged her, kept her calm, and helped her know when it was time to line up for her race. When the starting pistol sounded, she took off like a lightning bolt, leaving her fellow racers behind. As she neared the finish line, she stopped, turned around and motioned for the other runners to hurry. She waited for them so they could all cross the finish line together.'''\n",
        "\n",
        "tokens = re.findall(\"[\\w']+\",text)\n",
        "\n",
        "print(tokens)\n",
        "print(\"No.of tokens:\",len(tokens))\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['My', 'son', 'Mark', 'volunteered', 'to', 'help', 'Cherie', 'a', 'young', 'runner', 'at', 'a', 'local', 'Special', 'Olympics', 'Cherie', 'was', 'happy', 'and', 'enthusiastic', 'Mark', 'encouraged', 'her', 'kept', 'her', 'calm', 'and', 'helped', 'her', 'know', 'when', 'it', 'was', 'time', 'to', 'line', 'up', 'for', 'her', 'race', 'When', 'the', 'starting', 'pistol', 'sounded', 'she', 'took', 'off', 'like', 'a', 'lightning', 'bolt', 'leaving', 'her', 'fellow', 'racers', 'behind', 'As', 'she', 'neared', 'the', 'finish', 'line', 'she', 'stopped', 'turned', 'around', 'and', 'motioned', 'for', 'the', 'other', 'runners', 'to', 'hurry', 'She', 'waited', 'for', 'them', 'so', 'they', 'could', 'all', 'cross', 'the', 'finish', 'line', 'together']\n",
            "No.of tokens: 88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzhRNPk9yFWk",
        "outputId": "029a14bb-9524-4afb-8982-42d73fcf3d96"
      },
      "source": [
        "# SENTENCE TOKENIZATION\n",
        "\n",
        "sentences = re.compile('[.?!] ').split(text)\n",
        "\n",
        "print(sentences)\n",
        "print(\"No.of sentences:\",len(sentences))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['My son, Mark, volunteered to help Cherie, a young runner at a local Special Olympics', 'Cherie was happy and enthusiastic', 'Mark encouraged her, kept her calm, and helped her know when it was time to line up for her race', 'When the starting pistol sounded, she took off like a lightning bolt, leaving her fellow racers behind', 'As she neared the finish line, she stopped, turned around and motioned for the other runners to hurry', 'She waited for them so they could all cross the finish line together.']\n",
            "No.of sentences: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkQE4ly2um8B"
      },
      "source": [
        "### 3. Tokenization using NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZauXD5Ky-52",
        "outputId": "b1d2b9ff-eb8e-4d1c-8c79-9e05b7ece584"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUSrW5jXzGxm",
        "outputId": "5fae4410-8211-4a83-b7af-933d2ca99e75"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daaZBmLMuI4E",
        "outputId": "88c61854-3123-41af-a626-c26e068ef910"
      },
      "source": [
        "#Word tokenization\n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(tokens)\n",
        "\n",
        "print(\"No.of tokens : \", len(tokens))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['My', 'son', ',', 'Mark', ',', 'volunteered', 'to', 'help', 'Cherie', ',', 'a', 'young', 'runner', 'at', 'a', 'local', 'Special', 'Olympics', '.', 'Cherie', 'was', 'happy', 'and', 'enthusiastic', '.', 'Mark', 'encouraged', 'her', ',', 'kept', 'her', 'calm', ',', 'and', 'helped', 'her', 'know', 'when', 'it', 'was', 'time', 'to', 'line', 'up', 'for', 'her', 'race', '.', 'When', 'the', 'starting', 'pistol', 'sounded', ',', 'she', 'took', 'off', 'like', 'a', 'lightning', 'bolt', ',', 'leaving', 'her', 'fellow', 'racers', 'behind', '.', 'As', 'she', 'neared', 'the', 'finish', 'line', ',', 'she', 'stopped', ',', 'turned', 'around', 'and', 'motioned', 'for', 'the', 'other', 'runners', 'to', 'hurry', '.', 'She', 'waited', 'for', 'them', 'so', 'they', 'could', 'all', 'cross', 'the', 'finish', 'line', 'together', '.']\n",
            "No.of tokens :  103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLeLxfXGzedB",
        "outputId": "de303d52-507c-4a15-a8a0-85cc6b16a952"
      },
      "source": [
        "#Sentence tokenization\n",
        "from nltk.tokenize import sent_tokenize \n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "print(sentences)\n",
        "\n",
        "print(\"No.of sentences : \", len(sentences))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['My son, Mark, volunteered to help Cherie, a young runner at a local Special Olympics.', 'Cherie was happy and enthusiastic.', 'Mark encouraged her, kept her calm, and helped her know when it was time to line up for her race.', 'When the starting pistol sounded, she took off like a lightning bolt, leaving her fellow racers behind.', 'As she neared the finish line, she stopped, turned around and motioned for the other runners to hurry.', 'She waited for them so they could all cross the finish line together.']\n",
            "No.of sentences :  6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0qwbkAQ0N61"
      },
      "source": [
        "2.Perform **stemming** on the tokens present in the given sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3LLsyQ9JXjc",
        "outputId": "f80e8068-7a28-4bbe-cb0c-987b8ae72155"
      },
      "source": [
        "# STEMMING\n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "sentence=\"Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.\"\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "def stemSentence(sentence):\n",
        "    token_words=word_tokenize(sentence)\n",
        "    print(token_words)\n",
        "    stem_sentence=[]\n",
        "    for word in token_words:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    return \"\".join(stem_sentence)\n",
        "\n",
        "x=stemSentence(sentence)\n",
        "print(\"Sentence after stemming :\", x)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Stemming', 'is', 'the', 'process', 'of', 'reducing', 'inflection', 'in', 'words', 'to', 'their', 'root', 'forms', 'such', 'as', 'mapping', 'a', 'group', 'of', 'words', 'to', 'the', 'same', 'stem', 'even', 'if', 'the', 'stem', 'itself', 'is', 'not', 'a', 'valid', 'word', 'in', 'the', 'Language', '.']\n",
            "Sentence after stemming : stem is the process of reduc inflect in word to their root form such as map a group of word to the same stem even if the stem itself is not a valid word in the languag . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CrIXGNN2V6m"
      },
      "source": [
        "3.Perform **Lemmatization** on the tokens present in the given sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNe_-k1sMC8I",
        "outputId": "a0019e3d-2d5e-4571-c15d-da7feec690f5"
      },
      "source": [
        "#FIRST WE NEED DOWNLOAD THE WORDNET\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8tKDcgJMAs5",
        "outputId": "cda3c9bd-944c-4eb8-eadc-a09626fd67e2"
      },
      "source": [
        "#LEMMATIZATION\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = \"Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma.\"\n",
        "punctuations=\"?:!.,;\"\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "def lemmatizeSentence(sentence):\n",
        "    token_words = nltk.word_tokenize(sentence)\n",
        "    print(token_words)\n",
        "    lemma_sentence=[]\n",
        "    for word in token_words:\n",
        "      lemma_sentence.append(wordnet_lemmatizer.lemmatize(word))\n",
        "      lemma_sentence.append(\" \")\n",
        "\n",
        "    return \"\".join(lemma_sentence)\n",
        "\n",
        "y=lemmatizeSentence(sentence)\n",
        "print(\"Sentence after lemmatizing:\",y)\n",
        "\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lemmatization', ',', 'unlike', 'Stemming', ',', 'reduces', 'the', 'inflected', 'words', 'properly', 'ensuring', 'that', 'the', 'root', 'word', 'belongs', 'to', 'the', 'language', '.', 'In', 'Lemmatization', 'root', 'word', 'is', 'called', 'Lemma', '.']\n",
            "Sentence after lemmatizing: Lemmatization , unlike Stemming , reduces the inflected word properly ensuring that the root word belongs to the language . In Lemmatization root word is called Lemma . \n"
          ]
        }
      ]
    }
  ]
}